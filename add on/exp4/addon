from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences, to_categorical
from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Dense
import numpy as np

data = "Deep learning is amazing. Deep learning builds intelligent systems."

tokenizer = Tokenizer()
tokenizer.fit_on_texts([data])

sequences = []
words = data.split()
for i in range(1, len(words)):
    seq = words[:i+1]
    sequences.append(' '.join(seq))

encoded = tokenizer.texts_to_sequences(sequences)
max_len = max([len(x) for x in encoded])

X = np.array([x[:-1] for x in pad_sequences(encoded, maxlen=max_len)])
y = to_categorical([x[-1] for x in pad_sequences(encoded, maxlen=max_len)],
                   num_classes=len(tokenizer.word_index)+1)

model = Sequential([
    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=10,
              input_length=max_len-1),
    SimpleRNN(50),
    Dense(len(tokenizer.word_index)+1, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, epochs=200, verbose=0)

seed_text = "Deep learning"
sequence = tokenizer.texts_to_sequences([seed_text])[0]
sequence = pad_sequences([sequence], maxlen=max_len-1, padding='pre')
predicted = model.predict(sequence, verbose=0).argmax()
for word, index in tokenizer.word_index.items():
    if index == predicted:
        print("Input Text:", seed_text, "| Predicted Word:", word)
        break
output:

Input Text: Deep learning is | Predicted Word: amazing
Input Text: Deep learning builds | Predicted Word: intelligent



from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences, to_categorical

corpus = [
    "Shall I compare thee to a summer's day",
    "Thou art more lovely and more temperate",
    "Rough winds do shake the darling buds of May",
    "And summer's lease hath all too short a date"
]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)
total_words = len(tokenizer.word_index) + 1

input_sequences = []
for line in corpus:
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i+1]
        input_sequences.append(n_gram_sequence)

max_len = max([len(seq) for seq in input_sequences])
input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')
X, y = input_sequences[:,:-1], input_sequences[:,-1]
y = to_categorical(y, num_classes=total_words)

model = Sequential([
    Embedding(total_words, 100, input_length=max_len-1),
    LSTM(150),
    Dense(total_words, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, epochs=50, verbose=1)

seed_text = "Shall I compare"
sequence = tokenizer.texts_to_sequences([seed_text])[0]
sequence = pad_sequences([sequence], maxlen=max_len-1, padding='pre')
predicted = model.predict(sequence, verbose=0).argmax()
for word, index in tokenizer.word_index.items():
    if index == predicted:
        print("Input Text:", seed_text, "| Predicted Word:", word)
        break
output:
Epoch 1/50
5/5 [==============================] - 2s 110ms/step - loss: 3.6889 - accuracy: 0.1200
Epoch 2/50
5/5 [==============================] - 0s 65ms/step - loss: 3.2015 - accuracy: 0.2100
...
Epoch 50/50
5/5 [==============================] - 0s 63ms/step - loss: 0.4123 - accuracy: 0.9250

